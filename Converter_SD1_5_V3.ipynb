{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# **SD 1.5 Model Converter**\n",
        "\n",
        " **A Colab Notebook To Convert SD 1.5 Checkpoint to Diffusers format**\n",
        "\n",
        "But a horribly duct taped edition. THIS IS IN ALPHA STAGES, WILL BE PATCHING THE CODE AS I GO ALONG.\n",
        "\n",
        "---\n",
        "Link to this colab: https://colab.research.google.com/github/duskfallcrew/sd15-to-diffusers/blob/main/Converter_SD1_5_V2_FIXED.ipynb\n",
        "\n",
        "***Patched from*** : https://colab.research.google.com/github/Linaqruf/sdxl-model-converter/blob/main/sdxl_model_converter.ipynb\n",
        "\n",
        "\n",
        "RIGHT NOW THE INSTRUCTIONS ARE AS FOLLOWS:\n",
        "\n",
        "♻ - Install/Clone etc\n",
        "\n",
        "♻ - Download model - Direct port from Linaqruf.\n",
        "\n",
        "♻ - Put your model details in.\n",
        "\n",
        "♻ - DUMP PATH = folder name\n",
        "\n",
        "♻ - If your downloaded model is safetensors please tick the box.\n",
        "\n",
        "♻ - Check file browser, if the model/yourmodelhere looks like a diffusers format you're good to go!\n",
        "\n",
        "♻ - Write Token + Set up your Repo!\n",
        "\n",
        "♻ - Upload Diffusers!\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Ahoy! you're looking for our Huggingface backup that is again patched from Linaqruf and others?\n",
        "\n",
        "| Notebook Name | Description | Link |\n",
        "| --- | --- | --- |\n",
        "| [Huggingface Backup](https://colab.research.google.com/github/kieranxsomer/HuggingFace_Backup/blob/main/HuggingFace_Backup.ipynb) | backup checkpoints! | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/kieranxsomer/HuggingFace_Backup/blob/main/HuggingFace_Backup.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## Duskfall/ Earth & Dusk Socials\n",
        "\n",
        "Discord:\n",
        "![Discord](https://img.shields.io/discord/1024442483750490222?label=Earth%26Dusk&style=plastic)\n",
        "\n",
        "---\n",
        "\n",
        "| Social Network |  Link |\n",
        "| --- |  --- |\n",
        "|Discord|[Invite](https://discord.gg/5t2kYxt7An)\n",
        "|CivitAi|[Duskfallcrew](https://civitai.com/user/duskfallcrew/)\n",
        "|Huggingface|[Earth & Dusk](https://huggingface.co/EarthnDusk)\n",
        "|Ko-Fi| [Dusk's Kofi](https://ko-fi.com/duskfallcrew/)\n",
        "\n",
        "---\n",
        "\n",
        "***Linaqruf @ Github***: https://github.com/Linaqruf\n",
        "\n",
        " [![](https://dcbadge.vercel.app/api/shield/850007095775723532?style=flat)](https://lookup.guru/850007095775723532) [![ko-fi](https://img.shields.io/badge/Support%20me%20on%20Ko--fi-F16061?logo=ko-fi&logoColor=white&style=flat)](https://ko-fi.com/linaqruf) <a href=\"https://saweria.co/linaqruf\"><img alt=\"Saweria\" src=\"https://img.shields.io/badge/Saweria-7B3F00?style=flat&logo=ko-fi&logoColor=white\"/></a>\n",
        "\n",
        "---\n",
        "\n",
        " **Please use their main scripts for SDXL HERE:**\n",
        "\n",
        "| Notebook Name | Description | Link |\n",
        "| --- | --- | --- |\n",
        "| [Kohya LoRA Trainer XL](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-trainer-XL.ipynb) | LoRA Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-trainer-XL.ipynb) |\n",
        "| [Kohya Trainer XL](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer-XL.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer-XL.ipynb) |\n",
        "\n",
        "---\n",
        "SD 1.5 Scripts:\n",
        "\n",
        "| Notebook Name | Description | Link | V14 |\n",
        "| --- | --- | --- | --- |\n",
        "| [Kohya LoRA Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) | LoRA Training (Dreambooth method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-dreambooth.ipynb) | [![](https://img.shields.io/static/v1?message=Older%20Version&logo=googlecolab&labelColor=5c5c5c&color=e74c3c&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/ff701379c65380c967cd956e4e9e8f6349563878/kohya-LoRA-dreambooth.ipynb) |\n",
        "| [Kohya LoRA Fine-Tuning](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) | LoRA Training (Fine-tune method) | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-LoRA-finetuner.ipynb) | [![](https://img.shields.io/static/v1?message=Older%20Version&logo=googlecolab&labelColor=5c5c5c&color=e74c3c&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/ff701379c65380c967cd956e4e9e8f6349563878/kohya-LoRA-finetuner.ipynb) |\n",
        "| [Kohya Trainer](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) | Native Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-trainer.ipynb) | [![](https://img.shields.io/static/v1?message=Older%20Version&logo=googlecolab&labelColor=5c5c5c&color=e74c3c&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/ff701379c65380c967cd956e4e9e8f6349563878/kohya-trainer.ipynb) |\n",
        "| [Kohya Dreambooth](https://github.com/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) | Dreambooth Training | [![](https://img.shields.io/static/v1?message=Open%20in%20Colab&logo=googlecolab&labelColor=5c5c5c&color=0f80c1&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/main/kohya-dreambooth.ipynb) | [![](https://img.shields.io/static/v1?message=Older%20Version&logo=googlecolab&labelColor=5c5c5c&color=e74c3c&label=%20&style=flat)](https://colab.research.google.com/github/Linaqruf/kohya-trainer/blob/ff701379c65380c967cd956e4e9e8f6349563878/kohya-dreambooth.ipynb) |\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "zIXBLRPPUM6g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Disclaimer:\n",
        "This doesn't run the inference like the SDXL one, we don't understand how to patch that.\n",
        "\n",
        "IN THEORY This should work on anything that uses Jupyter, we'll test it later on via Vast or Runpod, and if you see this line edited eventually? You know it's working!\n",
        "\n",
        "Duskfallcrew/Earth and Dusk take no responsibility for the horribly patched code. Nor likely does Linaqruf. Do not smack us, do not sue us, don't sue Linaqruf!\n",
        "\n",
        "Updates:\n",
        "\n",
        "Patched the conversion, added snarky comments to the outputs.\n",
        "Our personal one has funny notes, feel free to patch and change at will.\n",
        "\n",
        "Added a fix keys for models that won't convert, and also added a patch for cleaning folders."
      ],
      "metadata": {
        "id": "U2DWCCaBDgni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## ♻ **Install Diffusers**\n",
        "import os\n",
        "from subprocess import getoutput\n",
        "\n",
        "!apt install aria2\n",
        "!pip install diffusers\n",
        "!pip install omegaconf\n",
        "!pip install transformers\n",
        "!pip install xformers\n",
        "!pip install accelerate\n",
        "!git clone https://github.com/huggingface/diffusers\n",
        "\n",
        "# Directories\n",
        "root_dir = \"/content\"\n",
        "repo_dir = os.path.join(root_dir, \"diffusers\")\n",
        "models_dir = os.path.join(root_dir, \"models\")\n",
        "vae_dir = os.path.join(root_dir, \"vae\")\n",
        "\n",
        "def prepare_environment():\n",
        "    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "    os.environ[\"SAFETENSORS_FAST_GPU\"] = \"1\"\n",
        "    os.environ[\"PYTHONWARNINGS\"] = \"ignore\"\n",
        "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"garbage_collection_threshold:0.9,max_split_size_mb:512\"\n",
        "    os.environ[\"TCMALLOC_AGGRESSIVE_DECOMMIT\"] = \"t\"\n",
        "    os.environ[\"CUDA_MODULE_LOADING\"] = \"LAZY\"\n",
        "\n",
        "def main():\n",
        "    os.chdir(root_dir)\n",
        "    os.chdir(repo_dir)\n",
        "    prepare_environment()\n",
        "\n",
        "def main():\n",
        "    print(f\"♻ Deployment Complete, Head to downloading your model please.\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "zsHNL7thg2WE",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## ♻ **Download SD 1.5**\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import glob\n",
        "import gdown\n",
        "import requests\n",
        "import subprocess\n",
        "from urllib.parse import urlparse, unquote\n",
        "from pathlib import Path\n",
        "\n",
        "os.chdir(root_dir)\n",
        "\n",
        "# @markdown Place your Huggingface [Read Token](https://huggingface.co/settings/tokens) Here.\n",
        "\n",
        "HUGGINGFACE_TOKEN = \"yourreadtokengoeshere\"#@param {type: \"string\"}\n",
        "SD_MODEL_URL = \"https whatever your model link is here\" #@param {type: \"string\"}\n",
        "\n",
        "\n",
        "def get_supported_extensions():\n",
        "    return tuple([\".ckpt\", \".safetensors\", \".pt\", \".pth\"])\n",
        "\n",
        "def get_filename(url):\n",
        "    extensions = get_supported_extensions()\n",
        "\n",
        "    if url.endswith(tuple(extensions)):\n",
        "        filename = os.path.basename(url)\n",
        "    else:\n",
        "        response = requests.get(url, stream=True)\n",
        "        response.raise_for_status()\n",
        "\n",
        "        if 'content-disposition' in response.headers:\n",
        "            content_disposition = response.headers['content-disposition']\n",
        "            filename = re.findall('filename=\"?([^\"]+)\"?', content_disposition)[0]\n",
        "        else:\n",
        "            url_path = urlparse(url).path\n",
        "            filename = unquote(os.path.basename(url_path))\n",
        "\n",
        "    if filename.endswith(tuple(get_supported_extensions())):\n",
        "        return filename\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "def parse_args(config):\n",
        "    args = []\n",
        "\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args.append(f\"{v}\")\n",
        "        elif isinstance(v, str) and v is not None:\n",
        "            args.append(f'--{k}={v}')\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args.append(f\"--{k}\")\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args.append(f\"--{k}={v}\")\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args.append(f\"--{k}={v}\")\n",
        "\n",
        "    return args\n",
        "\n",
        "def aria2_download(dir, filename, url):\n",
        "    user_header = f\"Authorization: Bearer {HUGGINGFACE_TOKEN}\"\n",
        "\n",
        "    aria2_config = {\n",
        "        \"console-log-level\"         : \"error\",\n",
        "        \"summary-interval\"          : 10,\n",
        "        \"header\"                    : user_header if \"huggingface.co\" in url else None,\n",
        "        \"continue\"                  : True,\n",
        "        \"max-connection-per-server\" : 16,\n",
        "        \"min-split-size\"            : \"1M\",\n",
        "        \"split\"                     : 16,\n",
        "        \"dir\"                       : dir,\n",
        "        \"out\"                       : filename,\n",
        "        \"_url\"                      : url,\n",
        "    }\n",
        "    aria2_args = parse_args(aria2_config)\n",
        "    subprocess.run([\"aria2c\", *aria2_args])\n",
        "\n",
        "def gdown_download(url, dst, filepath):\n",
        "    if \"/uc?id/\" in url or \"/file/d/\" in url:\n",
        "        return gdown.download(url, filepath, quiet=False)\n",
        "    elif \"/drive/folders/\" in url:\n",
        "        os.chdir(dst)\n",
        "        return gdown.download_folder(url, quiet=True, use_cookies=False)\n",
        "\n",
        "def download(url, dst):\n",
        "    filename = get_filename(url)\n",
        "    filepath = os.path.join(dst, filename)\n",
        "\n",
        "    if \"drive.google.com\" in url:\n",
        "        gdown = gdown_download(url, dst, filepath)\n",
        "    elif url.startswith(\"/content/drive/MyDrive/\"):\n",
        "        return url\n",
        "    else:\n",
        "        if \"huggingface.co\" in url:\n",
        "            if \"/blob/\" in url:\n",
        "                url = url.replace(\"/blob/\", \"/resolve/\")\n",
        "        aria2_download(dst, filename, url)\n",
        "\n",
        "def main():\n",
        "    model_path = SD_MODEL_URL\n",
        "    vae_path = VAE_MODEL_URL\n",
        "    download(model_path, models_dir)\n",
        "    print(f\"Model downloaded at: {model_path}\")\n",
        "\n",
        "def main():\n",
        "    print(f\"♻ Download complete, head to conversion.\")\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "id": "VAdeIoD6fsvU",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ## ♻ **Fix Before Converting**\n",
        "from safetensors import safe_open\n",
        "from safetensors.torch import save_file\n",
        "\n",
        "def fix_diffusers_model_conversion(load_path: str = None, save_path: str = None):\n",
        "    # Use default paths if not provided\n",
        "    load_path = load_path or \"input\"  # @param {type: \"string\"}\n",
        "    save_path = save_path or \"output\"  # @param {type: \"string\"}\n",
        "\n",
        "    # load original\n",
        "    tensors = {}\n",
        "    with safe_open(load_path, framework=\"pt\") as f:\n",
        "        for key in f.keys():\n",
        "            tensors[key] = f.get_tensor(key)\n",
        "\n",
        "    # migrate\n",
        "    new_tensors = {}\n",
        "    for k, v in tensors.items():\n",
        "        new_key = k\n",
        "        # only fix the vae\n",
        "        if 'first_stage_model.' in k:\n",
        "            # migrate q, k, v keys\n",
        "            new_key = new_key.replace('.to_q.weight', '.q.weight')\n",
        "            new_key = new_key.replace('.to_q.bias', '.q.bias')\n",
        "            new_key = new_key.replace('.to_k.weight', '.k.weight')\n",
        "            new_key = new_key.replace('.to_k.bias', '.k.bias')\n",
        "            new_key = new_key.replace('.to_v.weight', '.v.weight')\n",
        "            new_key = new_key.replace('.to_v.bias', '.v.bias')\n",
        "        new_tensors[new_key] = v\n",
        "\n",
        "    # save\n",
        "    save_file(new_tensors, save_path)\n",
        "\n",
        "# Example usage\n",
        "fix_diffusers_model_conversion(load_path='/content/drive/path/to/your/original_file.pth',\n",
        "                               save_path='/content/drive/path/to/save/fixed_file.pth')\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "c7ONmItJuKed"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### ♻ **Clean Folders for Optimized Space**\n",
        "\n",
        "from IPython.display import display, Markdown\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "# Function to clear and delete a folder\n",
        "def clear_and_delete_folder(colab_folder_path):\n",
        "    try:\n",
        "        # Use shutil.rmtree to remove all files and subdirectories\n",
        "        shutil.rmtree(colab_folder_path)\n",
        "        display(Markdown(f\"Deleted all contents in folder: `{colab_folder_path}`\"))\n",
        "    except Exception as e:\n",
        "        display(Markdown(f\"Error deleting folder `{colab_folder_path}`: {e}\"))\n",
        "\n",
        "# @markdown ### Folder Path for Deletion\n",
        "\n",
        "colab_folder_path = \"/content/models/yourinputhere\" # @param {type: \"string\"}\n",
        "\n",
        "# Call the function to clear and delete the folder\n",
        "clear_and_delete_folder(colab_folder_path)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dGHSzP-nvGEl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title ## ♻ **Convert SD 1.5 to Diffusers**\n",
        "\n",
        "import os\n",
        "import urllib.request\n",
        "from urllib.parse import urlparse, unquote\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "\n",
        "os.chdir(root_dir)\n",
        "#@markdown ### **Conversion Config**\n",
        "#@markdown Make sure you TICK SAFETENSORS or it won't convert.\n",
        "#@markdown Dump path is where you would like your model to be stored before uploading.\n",
        "\n",
        "checkpoint_path = \"/models/modelname.safetensors\" #@param {'type': 'string'}\n",
        "dump_path = \"/models/modelname\" #@param {'type': 'string'}\n",
        "from_safetensors = True  # @param {type: \"boolean\"}\n",
        "\n",
        "def convert_dict(config):\n",
        "    args = \"\"\n",
        "    for k, v in config.items():\n",
        "        if k.startswith(\"_\"):\n",
        "            args += f'\"{v}\" '\n",
        "        elif isinstance(v, str):\n",
        "            args += f'--{k}=\"{v}\" '\n",
        "        elif isinstance(v, bool) and v:\n",
        "            args += f\"--{k} \"\n",
        "        elif isinstance(v, float) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "        elif isinstance(v, int) and not isinstance(v, bool):\n",
        "            args += f\"--{k}={v} \"\n",
        "    return args\n",
        "\n",
        "\n",
        "\n",
        "def check_and_download_script(script_name, script_url):\n",
        "    if not os.path.exists(script_name):\n",
        "        print(f\"{script_name} not found, downloading...\")\n",
        "        urllib.request.urlretrieve(script_url, script_name)\n",
        "\n",
        "def run_script(script_name, script_args):\n",
        "    !python {script_name} {script_args}\n",
        "\n",
        "\n",
        "def main():\n",
        "    script_name = \"convert_original_stable_diffusion_to_diffusers.py\"\n",
        "    script_url = \"https://raw.githubusercontent.com/huggingface/diffusers/main/scripts/convert_original_stable_diffusion_to_diffusers.py\"\n",
        "    check_and_download_script(script_name, script_url)\n",
        "\n",
        "    config = {\n",
        "        \"checkpoint_path\": checkpoint_path,\n",
        "        \"dump_path\": dump_path,\n",
        "        \"from_safetensors\": from_safetensors,\n",
        "\n",
        "    }\n",
        "    run_script(script_name, convert_dict(config))\n",
        "    print(f\"♻ Conversion Succesful, head to upload!\")\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "id": "fYVYIFP8jDLJ",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### ♻ **Huggingface Hub config**\n",
        "from huggingface_hub import login, HfApi\n",
        "from huggingface_hub.utils import validate_repo_id, HfHubHTTPError\n",
        "\n",
        "# @markdown Login to Huggingface Hub\n",
        "# @markdown > Get **your** huggingface `WRITE` token [here](https://huggingface.co/settings/tokens)\n",
        "write_token = \"yourtokengoeshere\"  # @param {type:\"string\"}\n",
        "# @markdown Fill this if you want to upload to your organization, or just leave it empty.\n",
        "orgs_name = \"yourorganizationisoptinalitgoeshere\"  # @param{type:\"string\"}\n",
        "# @markdown If your model repo does not exist, it will automatically create it.\n",
        "model_name = \"modelname\"  # @param {type:\"string\"}\n",
        "make_private = False  # @param{type:\"boolean\"}\n",
        "\n",
        "def authenticate(write_token):\n",
        "    login(write_token, add_to_git_credential=True)\n",
        "    api = HfApi()\n",
        "    return api.whoami(write_token), api\n",
        "\n",
        "def create_model_repo(api, user, orgs_name, model_name, make_private=False):\n",
        "    if orgs_name == \"\":\n",
        "        repo_id = user[\"name\"] + \"/\" + model_name.strip()\n",
        "    else:\n",
        "        repo_id = orgs_name + \"/\" + model_name.strip()\n",
        "\n",
        "    try:\n",
        "        validate_repo_id(repo_id)\n",
        "        api.create_repo(repo_id=repo_id, repo_type=\"model\", private=make_private)\n",
        "        print(f\"Model repo '{repo_id}' didn't exist, creating repo\")\n",
        "    except HfHubHTTPError as e:\n",
        "        print(f\"♻ Repository existed,  '{repo_id}' exists, skipping create repo, head to upload.\")\n",
        "\n",
        "    print(f\"♻ Repository created, head to upload. Model repo '{repo_id}' link: https://huggingface.co/{repo_id}\\n\")\n",
        "\n",
        "    return repo_id\n",
        "\n",
        "user, api = authenticate(write_token)\n",
        "\n",
        "if model_name:\n",
        "    model_repo = create_model_repo(api, user, orgs_name, model_name, make_private)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "23u4u4XHfTu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title ### ♻ **Upload to Huggingface**\n",
        "from huggingface_hub import HfApi\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "api = HfApi()\n",
        "\n",
        "# @markdown This will be uploaded to model repo\n",
        "model_path = \"/content/models/yourinputhere\"  # @param {type :\"string\"}\n",
        "path_in_repo = \"\"  # @param {type :\"string\"}\n",
        "project_name = os.path.basename(model_path)\n",
        "\n",
        "# @markdown Other Information\n",
        "commit_message = \"Upload with \\uD83D\\uDE80\\uD83E\\uDD17 SD 1.5 Diffusers notebook\"  # @param {type :\"string\"}\n",
        "\n",
        "def is_diffusers_model(model_path):\n",
        "    required_folders = {\"unet\", \"text_encoder\", \"text_encoder_2\", \"tokenizer\", \"tokenizer_2\", \"scheduler\", \"vae\"}\n",
        "    return required_folders.issubset(set(os.listdir(model_path))) and os.path.isfile(os.path.join(model_path, \"model_index.json\"))\n",
        "\n",
        "def upload_model(model_paths, is_folder: bool):\n",
        "    path_obj = Path(model_paths)\n",
        "    trained_model = path_obj.parts[-1]\n",
        "\n",
        "    path_in_repo_local = path_in_repo if path_in_repo and not is_diffusers_model(model_paths) else \"\"\n",
        "\n",
        "    notification = f\"Uploading {trained_model} from {model_paths} to https://huggingface.co/{model_repo}\"\n",
        "    print(notification)\n",
        "\n",
        "    if is_folder:\n",
        "        if is_diffusers_model(model_paths):\n",
        "            commit_message = f\"Upload diffusers format: {trained_model}\"\n",
        "            print(\"Detected diffusers model. Adjusting upload parameters.\")\n",
        "        else:\n",
        "            commit_message = f\"Upload checkpoint: {trained_model}\"\n",
        "            print(\"Detected regular model. Adjusting upload parameters.\")\n",
        "\n",
        "        api.upload_folder(\n",
        "            folder_path=model_paths,\n",
        "            path_in_repo=path_in_repo_local,\n",
        "            repo_id=model_repo,\n",
        "            commit_message=commit_message,\n",
        "            ignore_patterns=\".ipynb_checkpoints\",\n",
        "        )\n",
        "    else:\n",
        "        commit_message = f\"Upload file: {trained_model}\"\n",
        "        api.upload_file(\n",
        "            path_or_fileobj=model_paths,\n",
        "            path_in_repo=path_in_repo_local,\n",
        "            repo_id=model_repo,\n",
        "            commit_message=commit_message,\n",
        "        )\n",
        "\n",
        "    success_notification = f\"♻ Model upload complete, care to try again? Thanks for flying Stable Diffusion Airlines, you owe me five dollars.. Check the model at https://huggingface.co/{model_repo}/tree/main\"\n",
        "    print(success_notification)\n",
        "\n",
        "def upload():\n",
        "    if model_path.endswith((\".ckpt\", \".safetensors\", \".pt\")):\n",
        "        upload_model(model_path, False)\n",
        "    else:\n",
        "        upload_model(model_path, True)\n",
        "\n",
        "upload()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xSsmLT6QfXhw",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}